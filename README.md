# ğŸ§  MCP Ollama Local

> **Web Local (FastAPI) + IA Local (Ollama) + Herramientas MCP**

![Python Version](https://img.shields.io/badge/python-3.13+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)
![CI](https://github.com/vladimiracunadev-create/mcp-ollama-local/actions/workflows/ci.yml/badge.svg?branch=main)

---

**mcp-ollama-local** es una plataforma ligera diseÃ±ada para ejecutar un entorno de chat con IA completamente local, integrando la potencia de **Ollama** con la flexibilidad del **Model Context Protocol (MCP)**.  

Permite conversar con LLMs locales (como `qwen`, `llama3`, etc.) y otorgarles capacidades reales mediante herramientas seguras (acceso a archivos, bÃºsqueda, informaciÃ³n del sistema), todo con persistencia en SQLite.

---

## ğŸš€ CaracterÃ­sticas Principales

- **Chat Conversacional**: Interfaz web limpia para interactuar con tus modelos locales.
- **Protocolo MCP**: IntegraciÃ³n nativa para expandir las capacidades del modelo (extensible).
- **Herramientas Seguras**: Incluye herramientas de sistema (`system_info`, `list_files`, `grep_text`).
- **Historial Persistente**: Guardado de conversaciones en SQLite.
- **Privacidad Total**: Todo corre en tu mÃ¡quina (`localhost`), ideal para datos sensibles.
- **Cloud Native**: Listo para Docker y Kubernetes (K8s) desde el primer dÃ­a.

## ğŸ›  Arquitectura

El flujo de informaciÃ³n es directo y local:

```mermaid
graph LR
    User["Usuario (Navegador)"] -->|HTTP| FastAPI["Backend FastAPI"]
    FastAPI -->|Check| SQLite[("Historial DB")]
    FastAPI -->|Prompt| Ollama["Ollama (LLM Local)"]
    Ollama -->|Call Tool?| MCP["MCP Server Tools"]
    MCP -->|Result| Ollama
    Ollama -->|Response| FastAPI
    FastAPI -->|HTML/JSON| User
```

## ğŸ’» Especificaciones del Sistema

Para garantizar un rendimiento fluido, especialmente al ejecutar modelos locales de IA, se recomiendan las siguientes especificaciones:

### Hardware Recomendado
- **Procesador**: Apple Silicon (M1/M2/M3) o CPU robusta con soporte AVX2.
- **RAM**:
  - MÃ­nimo: **8 GB** (para modelos cuantizados de 7B parÃ¡metros o menores).
  - Recomendado: **16 GB** o mÃ¡s (para modelos de 7B+ con mayor contexto).
- **Almacenamiento**: ~10 GB libres para modelos y dependencias.

### Requisitos de Software
- **Sistema Operativo**: macOS (optimizado), Linux, o Windows (vÃ­a WSL2).
- **Python**: VersiÃ³n **3.13** o superior.
- **Ollama**: Ãšltima versiÃ³n estable para la ejecuciÃ³n del modelo.
- **Git**: Para clonar el repositorio.

## ğŸ“‹ Pre-requisitos de InstalaciÃ³n

1.  **Ollama**: Instalado y ejecutÃ¡ndose en segundo plano (`ollama serve`).
2.  **UV**: Gestor de paquetes de Python de alto rendimiento (altamente recomendado) o `pip` estÃ¡ndar.
3.  **Modelo**: Al menos un modelo descargado (ej. `ollama pull qwen2.5-coder:7b`).

## ğŸš€ Modos de Despliegue

Este proyecto es extremadamente flexible y puede desplegarse donde lo necesites. Para instrucciones detalladas, consulta [ğŸ“˜ GUÃA DE INSTALACIÃ“N (INSTALL.md)](INSTALL.md).

| Entorno | Ideal para... | Comando RÃ¡pido |
| :--- | :--- | :--- |
| **ğŸ’» Local** | Desarrollo, pruebas rÃ¡pidas | `make run` |
| **ğŸ³ Docker** | Uso aislado, producciÃ³n simple | `docker compose up -d` |
| **â˜¸ï¸ Kubernetes** | Escalado, clÃºsteres domÃ©sticos/prod | `kubectl apply -f k8s/` |

## âš¡ï¸ Inicio RÃ¡pido (Local)

Este proyecto incluye un `Makefile` para facilitar todas las tareas locales.

1.  **Clonar y configurar entorno**:
    ```bash
    git clone https://github.com/vladimiracunadev-create/mcp-ollama-local.git
    cd mcp-ollama-local
    make install
    ```

2.  **Asegurar que Ollama corre**:
    ```bash
    ollama serve
    # En otra terminal, descarga un modelo si no tienes uno
    ollama pull qwen2.5-coder:7b
    ```

3.  **Ejecutar la aplicaciÃ³n**:
    ```bash
    make run
    # O alternativamente: python main.py
    ```

4.  **Abrir en navegador**:
    Visita [http://localhost:8000](http://localhost:8000)

## ğŸ§° Comandos Disponibles

Usa `make help` para ver la lista completa. Los mÃ¡s comunes son:

| Comando | DescripciÃ³n |
| :--- | :--- |
| `make install` | Instala dependencias y configura el entorno virtual. |
| `make run` | Levanta el servidor de desarrollo en puerto 8000. |
| `make lint` | Verifica estilo y errores con **Ruff**. |
| `make format` | Corrige formato de cÃ³digo automÃ¡ticamente. |
| `make test` | Ejecuta las pruebas unitarias con **Pytest**. |
| `make clean` | Elimina archivos temporales y cachÃ©s. |

## ğŸ“‚ Estructura del Proyecto

```text
mcp-ollama-local/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ web/            # AplicaciÃ³n FastAPI y frontend
â”‚       â”œâ”€â”€ templates/  # Plantillas HTML (Jinja2)
â”‚       â””â”€â”€ static/     # Assets estÃ¡ticos (CSS, JS)
â”œâ”€â”€ mcp_server/         # ImplementaciÃ³n del servidor MCP y herramientas
â”œâ”€â”€ host/               # LÃ³gica de interacciÃ³n con entorno host
â”œâ”€â”€ data/               # Base de datos SQLite y logs
â”œâ”€â”€ main.py             # Punto de entrada alternativo
â”œâ”€â”€ Makefile            # Comandos de automatizaciÃ³n
â””â”€â”€ pyproject.toml      # ConfiguraciÃ³n de dependencias y herramientas
```

## ğŸ¤ Comunidad
Este proyecto se rige por un [CÃ³digo de Conducta](CODE_OF_CONDUCT.md). Al participar, se espera que mantengas este cÃ³digo.

- **Historial de Cambios**: Consulta el [CHANGELOG.md](CHANGELOG.md) para ver la evoluciÃ³n del proyecto.
- **CrÃ©ditos**: Conoce a los creadores en [AUTHORS.md](AUTHORS.md).
- **Contribuciones**: Lee [CONTRIBUTING.md](CONTRIBUTING.md) antes de enviar tu primer PR.

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo [LICENSE](LICENSE) para mÃ¡s informaciÃ³n.

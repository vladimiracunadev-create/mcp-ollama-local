# ğŸ§  MCP Ollama Local

> **Web Local (FastAPI) + IA Local (Ollama) + Herramientas MCP**

![Python Version](https://img.shields.io/badge/python-3.13+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)

---

**mcp-ollama-local** es una plataforma ligera diseÃ±ada para ejecutar un entorno de chat con IA completamente local, integrando la potencia de **Ollama** con la flexibilidad del **Model Context Protocol (MCP)**.  

Permite conversar con LLMs locales (como `qwen`, `llama3`, etc.) y otorgarles capacidades reales mediante herramientas seguras (acceso a archivos, bÃºsqueda, informaciÃ³n del sistema), todo con persistencia en SQLite.

---

## ğŸš€ CaracterÃ­sticas Principales

- **Chat Conversacional**: Interfaz web limpia para interactuar con tus modelos locales.
- **Protocolo MCP**: IntegraciÃ³n nativa para expandir las capacidades del modelo (extensible).
- **Herramientas Seguras**: Incluye herramientas de sistema (`system_info`, `list_files`, `grep_text`).
- **Historial Persistente**: Guardado de conversaciones en SQLite.
- **Privacidad Total**: Todo corre en tu mÃ¡quina (`localhost`), ideal para datos sensibles.

## ğŸ›  Arquitectura

El flujo de informaciÃ³n es directo y local:

```mermaid
graph LR
    User["Usuario (Navegador)"] -->|HTTP| FastAPI["Backend FastAPI"]
    FastAPI -->|Check| SQLite[("Historial DB")]
    FastAPI -->|Prompt| Ollama["Ollama (LLM Local)"]
    Ollama -->|Call Tool?| MCP["MCP Server Tools"]
    MCP -->|Result| Ollama
    Ollama -->|Response| FastAPI
    FastAPI -->|HTML/JSON| User
```

## ğŸ“‹ Requisitos Previos

1.  **Ollama**: Instalado y ejecutÃ¡ndose (`ollama serve`).
2.  **UV**: Gestor de paquetes de Python (recomendado) o Python 3.13+.
3.  **Modelo**: Al menos un modelo descargado (ej. `ollama pull qwen2.5-coder`).

## âš¡ï¸ Inicio RÃ¡pido

Este proyecto incluye un `Makefile` para facilitar todas las tareas.

1.  **Clonar y configurar entorno**:
    ```bash
    git clone https://github.com/vladimiracunadev-create/mcp-ollama-local.git
    cd mcp-ollama-local
    make install
    ```

2.  **Asegurar que Ollama corre**:
    ```bash
    ollama serve
    # En otra terminal, descarga un modelo si no tienes uno
    ollama pull qwen2.5-coder:7b
    ```

3.  **Ejecutar la aplicaciÃ³n**:
    ```bash
    make run
    # O alternativamente: python main.py
    ```

4.  **Abrir en navegador**:
    Visita [http://localhost:8000](http://localhost:8000)

## ğŸ§° Comandos Disponibles

Usa `make help` para ver la lista completa. Los mÃ¡s comunes son:

| Comando | DescripciÃ³n |
| :--- | :--- |
| `make install` | Instala dependencias y configura el entorno virtual. |
| `make run` | Levanta el servidor de desarrollo en puerto 8000. |
| `make lint` | Verifica estilo y errores con **Ruff**. |
| `make format` | Corrige formato de cÃ³digo automÃ¡ticamente. |
| `make test` | Ejecuta las pruebas unitarias con **Pytest**. |
| `make clean` | Elimina archivos temporales y cachÃ©s. |

## ğŸ“‚ Estructura del Proyecto

```text
mcp-ollama-local/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ web/            # AplicaciÃ³n FastAPI y frontend
â”‚       â”œâ”€â”€ templates/  # Plantillas HTML (Jinja2)
â”‚       â””â”€â”€ static/     # Assets estÃ¡ticos (CSS, JS)
â”œâ”€â”€ mcp_server/         # ImplementaciÃ³n del servidor MCP y herramientas
â”œâ”€â”€ host/               # LÃ³gica de interacciÃ³n con entorno host
â”œâ”€â”€ data/               # Base de datos SQLite y logs
â”œâ”€â”€ main.py             # Punto de entrada alternativo
â”œâ”€â”€ Makefile            # Comandos de automatizaciÃ³n
â””â”€â”€ pyproject.toml      # ConfiguraciÃ³n de dependencias y herramientas
```

## ğŸ¤ Contribuyendo

1.  Haz un Fork del repositorio.
2.  Crea una rama para tu feature (`git checkout -b feature/nueva-magia`).
3.  AsegÃºrate de que el cÃ³digo pase el linter (`make lint`).
4.  Haz Commit y Push.
5.  Abre un Pull Request.

Ver [CONTRIBUTING.md](CONTRIBUTING.md) para mÃ¡s detalles.

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Consulta el archivo [LICENSE](LICENSE) para mÃ¡s informaciÃ³n.
